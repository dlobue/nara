centralized search:

how the hell is this going to work without opening a billion search connections?

have each thing that needs to know if there are updates generate and remember their own queries?
submit signal with query, central search receives query, processes it, submits signal back with results?


may as well open search connections as they are required. close when finished in order to release resources.


new idea:
instead of including search connection management and query generation into everything that might need it, build a single class that handles it and then include that class into everything that needs it.

yay, best of both worlds!


so who keeps track of thread id and muuid lists? search mixin, or container?

to answer that, first i gotta know whether "Searchy" is a mixin that i subclass, or if it is to be an attribute of the container.


--

okay, assuming every object remembers their queries and builds those queries by themselves before they ask for the results. (pass the query to the object on init and it grabs its results after init)

problems:
-how do i merge in new results without duplicating existing data?
-how do i keep the number of search queries down to an absolute minimum?
   :: ie- one query to get the thread ids, and a second query to get all threads with those ids? no.

1. keep a record of all muuids currently in container. go through new result set and if data.id in _cache_muuids: pass

2. can't. not with current implementation of xappy. two will have to do.

--


signals = ['query', 'results', 'updates']

Signals.connect(self, 'query', CentralSearch.query, self)
Signals.connect(self, 'results', self.results)
Signals.connect(__main__, 'updates', self.updates)

def updates(self, newarrival):
	na_muuid,na_threadid = newarrival
	if na_threadid in self._threads:
		#do search for na_muuid
		#run results through msg_factory and conv_factory
		#merge new conv_factory into existing thread.